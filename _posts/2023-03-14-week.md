---
title: '周小结(2023.03.08-2023.03.14)'
date: 2023-03-14
permalink: /posts/2023-03-14_week/
---
| title                                                                                               | journal    | content                                                                                            | conclusion                                                                                        | gain                                                                                                                       |
|:----------------------------------------------------------------------------------------------------|:-----------|:---------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|
| Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting | NLPS 2019  | 提出了卷积自注意和LogSparse Transformer来解决Transformer在时序数据上的局部不可知论和内存瓶颈问题。 | 卷积自注意通过因果卷积提高局部上下文敏感性，LogSparse Transformer通过堆叠自注意力层减少内存成本。 | 在网络中复现实验，发现与GRU效果相当，支持并行，训练速度加快；理解注意力得分分布对不同频率信息的关注差异。                  |
| Conformer: Local Features Coupling Global Representations for Visual Recognition                    | arxiv 2021 | 提出了Conformer架构，通过FCU桥接卷积和Transformer分支，以交互方式融合局部和全局特征。              | Conformer采用并行且互补的双分支结构，增强全局语义和局部特征的交互，提升视觉识别性能。             | 实验发现单独Transformer难以训练，加入卷积特性后的融合结构表现更佳，考虑将图像网络的卷积结合Transformer策略应用于时序数据。 |

<embed src="/files/post/2023-03-14-week.pdf" type="application/pdf" height="400px" />
    