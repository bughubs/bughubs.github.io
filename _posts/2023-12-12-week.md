---
title: '周小结(2023.12.06-2023.12.12)'
date: 2023-12-12
permalink: /posts/2023-12-12_week/
---
| title                                                                            | journal    | content                                                                              | conclusion                                                                                     | gain                                                                                    |
|:---------------------------------------------------------------------------------|:-----------|:-------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|
| GRAPH-BERT: Only Attention is Needed for Learning Graph Representations          | Nips2020   | 提出一种仅依赖attention机制的图网络Graph-BERT，用于图表示学习。                      | Graph-BERT通过子图采样和三种position encoding，实现了基于图的预训练模型。                      | 了解了Graph-BERT如何解决大图处理难题和如何利用attention机制捕获节点角色信息。           |
| GraphiT: Encoding Graph Structure in Transformers                                | arXiv 2021 | GraphiT利用相对位置编码和local sub-structures编码，改进transformer在图结构上的表现。 | GraphiT通过特定策略将局部图结构信息编码进模型，提升了transformer在图数据上的性能。             | 认识到在图transformer中保留结构信息的重要性，以及如何通过编码策略实现这一点。           |
| GraphFormers:GNN-nested Transformers forRepresentation Learning on Textual Graph |            | 提出GraphFormers，一种深度整合GNN和Transformer的网络架构，用于文本图表示学习。       | GraphFormers通过层级化的PLM-GNN整合方式，在PLM编码阶段就引入邻域信息，提高了节点文本表示质量。 | 理解了GraphFormers如何克服级联架构的局限性，以及如何在PLM编码过程中有效利用图结构信息。 |


![image](/files/post/2023-12-12-week/0.jpg)
![image](/files/post/2023-12-12-week/1.jpg)
![image](/files/post/2023-12-12-week/2.jpg)
![image](/files/post/2023-12-12-week/3.jpg)