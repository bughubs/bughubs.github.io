---
title: '周小结(2023.12.06-2023.12.12)'
date: 2023-12-12
permalink: /posts/2023-12-12_week/
---
| title                                                                            | journal    | content                                                                            | conclusion                                                                | gain                                                                         |
|:---------------------------------------------------------------------------------|:-----------|:-----------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------------------------|
| GRAPH-BERT: Only Attention is Needed for Learning Graph Representations          | Nips2020   | 提出一种仅依赖attention机制的图网络Graph-BERT，用于图表示学习。                    | Graph-BERT通过子图采样和三种position encoding，实现了基于图的预训练模型。 | 了解了Graph-BERT如何解决大图处理难题，考虑在半监督工作中应用子图采样的方法。 |
| GraphiT: Encoding Graph Structure in Transformers                                | arXiv 2021 | GraphiT结合transformer与图结构信息，通过相对位置编码和局部子结构编码提升模型性能。 | GraphiT证明了全局通信的transformer也能在图任务中表现良好。                | 启发了在graph transformer中保留时序信息的可能性，需进一步寻找相关文献。      |
| GraphFormers:GNN-nested Transformers forRepresentation Learning on Textual Graph |            | 提出GraphFormers，一种深度整合GNN与PLM的网络架构，用于文本图表示学习。             | GraphFormers通过层级化整合PLM与GNN，提高了节点文本表示的质量。            | 可借鉴GraphFormers的思路，将PLM生成的图像信息与GNN融合，优化文本图表示。     |

<embed src="http://127.0.0.1:4000/files/post/2023-12-12-week.pdf" type="application/pdf" height="400px" />
    