---
title: '周小结(2022.10.25-2022.11.01)'
date: 2022-11-01
permalink: /posts/2022-11-01_week/
---
| title                                        | journal        | content                                                                                                                                | conclusion                                                                                                               | gain                                                                                                                                  |
|:---------------------------------------------|:---------------|:---------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------|
| Block-Recurrent Transformers                 | arXiv          | 结合了LSTM和Transformer的优点，提出了Block-Recurrent Transformers，通过循环单元以递归方式应用Transformer层，实现了对长序列的高效处理。 | Block-Recurrent Transformers通过引入循环单元和滑动注意力机制，解决了长序列处理的效率问题，适用于长文档的语言建模。       | 了解了Block-Recurrent Transformers如何结合LSTM和Transformer的优势，以及其在长序列建模上的创新点，包括滑动注意力机制和递归单元的设计。 |
| MsIFT: Multi-Source Image Fusion Transformer | Remote Sensing | 提出了一种多源图像融合变换器MsIFT，利用Transformer的全局注意力机制，增强了模型对空间错位的鲁棒性。                                     | MsIFT通过CNN特征提取器、特征融合变换器和任务预测器的组合，实现了多源图像的有效融合，特别适合于处理非精确配准的图像数据。 | 认识到MsIFT在多源图像融合方面的优势，特别是其特征融合Transformer结构和改进的loss函数，对于提升模型的融合能力和鲁棒性具有重要启示。    |

<embed src="http://127.0.0.1:4000/files/post/2022-11-01-week.pdf" type="application/pdf" height="400px" />
    