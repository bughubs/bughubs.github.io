---
title: '周小结(2022.12.14-2022.12.27)'
date: 2022-12-27
permalink: /posts/2022-12-27_week/
---
| title                                                                       | journal    | content                                                                               | conclusion                                                                           | gain                                                                                                               |
|:----------------------------------------------------------------------------|:-----------|:--------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|
| TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding | arXiv      | 研究了结合BiLSTM和Transformer创建更强大模型架构的方法，即TRANS-BLSTM。                | TRANS-BLSTM通过在每个Transformer块中集成BLSTM层，形成联合建模框架。                  | 了解了TRANS-BLSTM-1和TRANS-BLSTM-2的具体实现，以及如何处理BLSTM输出维度匹配问题。                                  |
| Attention Bottlenecks for Multimodal Fusion                                 | arXiv:2022 | 提出了一种基于Transformer的多层融合方法，利用'fusion bottlenecks'改善多模态融合性能。 | 该方法通过跨模态注意力机制和部分token的cross-attention降低了计算量并提高了融合效果。 | 认识到处理多模态问题时，应考虑单模态内部信息的充分提取，并理解了mid fusion策略的优势及跨模态注意力机制的设计思路。 |

<embed src="/files/post/2022-12-27-week.pdf" type="application/pdf" height="400px" />
    