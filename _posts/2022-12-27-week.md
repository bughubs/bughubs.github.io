---
title: '周小结(2022.12.14-2022.12.27)'
date: 2022-12-27
permalink: /posts/2022-12-27_week/
---
| title                                                                       | journal    | content                                                                                 | conclusion                                                                             | gain                                                                        |
|:----------------------------------------------------------------------------|:-----------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------|
| TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding | arXiv      | 研究了结合BiLSTM和Transformer创建更强大模型架构的方法，即TRANS-BLSTM。                  | TRANS-BLSTM通过在每个Transformer块中集成BLSTM层，形成联合建模框架。                    | 了解了TRANS-BLSTM-1和TRANS-BLSTM-2两种变体，以及它们如何处理维度匹配问题。  |
| Attention Bottlenecks for Multimodal Fusion                                 | arXiv:2022 | 提出了一种基于Transformer的多层融合方法，利用'fusion bottlenecks'来优化多模态信息融合。 | 该方法通过跨模态注意力机制和部分token的cross-attention，降低了计算量并提高了融合效果。 | 理解了mid fusion策略以及如何在多模态Transformer中平衡低层和高层特征的学习。 |


![image](/files/post/2022-12-27-week/0.jpg)
![image](/files/post/2022-12-27-week/1.jpg)
![image](/files/post/2022-12-27-week/2.jpg)
![image](/files/post/2022-12-27-week/3.jpg)